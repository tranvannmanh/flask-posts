{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pyvi import ViTokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import sqlite3\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "con = sqlite3.connect('../../instance/recommend_system.sqlite')\n",
    "db_env = con.cursor()\n",
    "\n",
    "stopwords_path = './vietnamese-stopword-dash.txt'\n",
    "save_path = './models'\n",
    "PATH_LDA_MODEL = f'{save_path}/LDA.model'\n",
    "PATH_CORPUS = f'{save_path}/CORPUS.mm'\n",
    "PATH_TOPICS_DOCS_DIST = f'{save_path}/topics_docs_dist.dat'\n",
    "PATH_DICTIONARY = f'{save_path}/id2word.dictionary'\n",
    "\n",
    "def jensen_shannon(doc_distribute, matrix_distribute):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M (the number of documents in the corpus)\n",
    "    \"\"\"\n",
    "    p = doc_distribute[None, :].T\n",
    "    q = matrix_distribute.T\n",
    "    m = .5 * (p + q)\n",
    "    return np.sqrt(.5 * (entropy(q, m) + entropy(p, m)))\n",
    "\n",
    "def get_most_similar_news(doc_distribute, matrix_distribute, k=10):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and returns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    # List of jensen-shannon distances\n",
    "    sims = jensen_shannon(doc_distribute=doc_distribute, matrix_distribute=matrix_distribute)\n",
    "\n",
    "    # return index of most K similar distribution from list\n",
    "    return np.argsort(sims)[:k]\n",
    "\n",
    "def stopwords(text_file_path=stopwords_path):\n",
    "    _stopwords = list(open(text_file_path, encoding='utf8').read().split())\n",
    "    return _stopwords\n",
    "\n",
    "_stopwords_default = stopwords()\n",
    "def remove_stopwords(text, stopwords=_stopwords_default):\n",
    "    return [word for word in text.split() if word not in stopwords]\n",
    "\n",
    "def remove_numeric(text):\n",
    "    table = str.maketrans({key: None for key in string.digits})\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_emails(text):\n",
    "    return re.sub('\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def remove_multiple_whitespace(text):\n",
    "    return re.sub(\"\\s\\s+\", \" \", text)\n",
    "\n",
    "def remove_newline_characters(text):\n",
    "    return re.sub('\\n', ' ', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"https://stackoverflow.com/a/37221663\"\"\"\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    return text.translate(table)\n",
    "\n",
    "def vi_tokenizer(text):\n",
    "    return ViTokenizer.tokenize(text)\n",
    "\n",
    "def simple_preprocessing(text):\n",
    "    _text = remove_newline_characters(text.lower())\n",
    "    _text = remove_emails(_text)\n",
    "    _text = remove_links(_text)\n",
    "    _text = remove_numeric(_text)\n",
    "    _text = remove_punctuation(_text)\n",
    "    _text = remove_multiple_whitespace(_text)\n",
    "    _text = vi_tokenizer(_text)\n",
    "    _text = remove_stopwords(_text)\n",
    "    return _text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_env.execute('SELECT content FROM NEWS')\n",
    "data = db_env.fetchall()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6764\n"
     ]
    }
   ],
   "source": [
    "docs_token = [simple_preprocessing(text[0]) for text in data]\n",
    "# print(docs_token)\n",
    "print(len(docs_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(docs_token)\n",
    "corpus = [id2word.doc2bow(doc_token) for doc_token in docs_token]\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=50, passes=10)\n",
    "if os.path.isdir(save_path):\n",
    "    lda_model.save(PATH_LDA_MODEL)\n",
    "else:\n",
    "    os.mkdir(save_path)\n",
    "    lda_model.save(PATH_LDA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31,\n",
       "  '0.054*\"câu\" + 0.033*\"kính\" + 0.021*\"tường\" + 0.019*\"cửa\" + 0.018*\"đáp_án\" + 0.017*\"phòng\" + 0.016*\"gạch\" + 0.013*\"hình\" + 0.013*\"độc_giả\" + 0.012*\"cửa_sổ\"'),\n",
       " (35,\n",
       "  '0.045*\"nga\" + 0.042*\"ukraine\" + 0.024*\"tên_lửa\" + 0.015*\"mỹ\" + 0.012*\"lan\" + 0.010*\"vụ\" + 0.009*\"lực_lượng\" + 0.008*\"tổng_thống\" + 0.008*\"quân_sự\" + 0.007*\"triều_tiên\"'),\n",
       " (4,\n",
       "  '0.040*\"quang\" + 0.034*\"liêm\" + 0.033*\"thanh_tra\" + 0.030*\"lý\" + 0.027*\"ván\" + 0.025*\"đóng\" + 0.019*\"linh\" + 0.015*\"phi\" + 0.015*\"hong\" + 0.014*\"kong\"'),\n",
       " (47,\n",
       "  '0.144*\"tim\" + 0.038*\"nhịp\" + 0.027*\"huyền\" + 0.020*\"dubai\" + 0.018*\"suy\" + 0.018*\"tim_mạch\" + 0.016*\"huyết_áp\" + 0.013*\"động_mạch\" + 0.013*\"băng\" + 0.011*\"ngực\"'),\n",
       " (20,\n",
       "  '0.033*\"lao_động\" + 0.024*\"châu\" + 0.021*\"công_ty\" + 0.021*\"usd\" + 0.020*\"âu\" + 0.017*\"triệu\" + 0.016*\"hàng\" + 0.014*\"nga\" + 0.013*\"công_nhân\" + 0.013*\"giá\"'),\n",
       " (0,\n",
       "  '0.038*\"nghiên_cứu\" + 0.037*\"khoa_học\" + 0.035*\"công_nghệ\" + 0.023*\"vaccine\" + 0.021*\"phát_triển\" + 0.019*\"sản_phẩm\" + 0.018*\"tiêm\" + 0.015*\"sản_xuất\" + 0.015*\"trồng\" + 0.014*\"b\"'),\n",
       " (29,\n",
       "  '0.035*\"đất\" + 0.030*\"khu\" + 0.028*\"dự_án\" + 0.025*\"xây_dựng\" + 0.019*\"phát_triển\" + 0.016*\"đô_thị\" + 0.014*\"quy_hoạch\" + 0.013*\"đầu_tư\" + 0.011*\"thành_phố\" + 0.011*\"công_trình\"'),\n",
       " (30,\n",
       "  '0.045*\"mây\" + 0.035*\"bạc\" + 0.033*\"chiến\" + 0.031*\"bom\" + 0.021*\"liêu\" + 0.020*\"cam\" + 0.019*\"texas\" + 0.018*\"áo_mưa\" + 0.016*\"đám\" + 0.014*\"kg\"'),\n",
       " (23,\n",
       "  '0.020*\"tiền\" + 0.015*\"quy_định\" + 0.015*\"thông_tin\" + 0.014*\"quyền\" + 0.014*\"giá\" + 0.013*\"đồng\" + 0.012*\"đấu_giá\" + 0.011*\"mua\" + 0.011*\"trường_hợp\" + 0.011*\"chủ\"'),\n",
       " (7,\n",
       "  '0.062*\"vàng\" + 0.049*\"siêu\" + 0.033*\"usd\" + 0.024*\"mỹ\" + 0.018*\"lạm_phát\" + 0.016*\"fed\" + 0.012*\"black\" + 0.010*\"lãi_suất\" + 0.009*\"adam\" + 0.009*\"giá\"'),\n",
       " (43,\n",
       "  '0.029*\"du_lịch\" + 0.019*\"bay\" + 0.019*\"du_khách\" + 0.018*\"chuyến\" + 0.017*\"đi\" + 0.016*\"vé\" + 0.012*\"phòng\" + 0.012*\"khách_sạn\" + 0.010*\"ảnh\" + 0.010*\"tàu\"'),\n",
       " (17,\n",
       "  '0.052*\"chạy\" + 0.030*\"marathon\" + 0.030*\"giải\" + 0.026*\"runner\" + 0.017*\"vnexpress\" + 0.014*\"vm\" + 0.014*\"hanoi\" + 0.013*\"midnight\" + 0.013*\"đua\" + 0.012*\"km\"'),\n",
       " (48,\n",
       "  '0.051*\"học\" + 0.020*\"chương_trình\" + 0.019*\"sinh_viên\" + 0.019*\"trường\" + 0.016*\"mỹ\" + 0.014*\"đại_học\" + 0.014*\"ngành\" + 0.013*\"học_bổng\" + 0.012*\"ứng_viên\" + 0.009*\"học_sinh\"'),\n",
       " (8,\n",
       "  '0.060*\"đức\" + 0.045*\"nhật\" + 0.036*\"pháp\" + 0.033*\"tây_ban\" + 0.029*\"nha\" + 0.015*\"world_cup\" + 0.015*\"italy\" + 0.011*\"tuyển\" + 0.009*\"costa\" + 0.009*\"rica\"'),\n",
       " (19,\n",
       "  '0.028*\"bác_sĩ\" + 0.021*\"bệnh\" + 0.020*\"điều_trị\" + 0.018*\"bệnh_nhân\" + 0.018*\"bệnh_viện\" + 0.013*\"ung_thư\" + 0.013*\"người_bệnh\" + 0.012*\"thuốc\" + 0.011*\"đau\" + 0.010*\"máu\"'),\n",
       " (44,\n",
       "  '0.040*\"trận\" + 0.027*\"thắng\" + 0.027*\"hai\" + 0.017*\"bàn\" + 0.017*\"argentina\" + 0.016*\"vòng\" + 0.016*\"bóng\" + 0.014*\"thua\" + 0.014*\"bảng\" + 0.013*\"phút\"'),\n",
       " (15,\n",
       "  '0.035*\"biển\" + 0.015*\"dân\" + 0.013*\"m\" + 0.012*\"đông\" + 0.012*\"bắc\" + 0.012*\"huyện\" + 0.011*\"xã\" + 0.011*\"bão\" + 0.010*\"mưa\" + 0.010*\"núi\"'),\n",
       " (42,\n",
       "  '0.089*\"the\" + 0.033*\"i\" + 0.032*\"a\" + 0.030*\"in\" + 0.028*\"of\" + 0.025*\"to\" + 0.019*\"ví_dụ\" + 0.017*\"my\" + 0.016*\"and\" + 0.016*\"you\"'),\n",
       " (40,\n",
       "  '0.055*\"tàu\" + 0.032*\"hữu\" + 0.031*\"điện\" + 0.017*\"xăng\" + 0.016*\"đồng\" + 0.016*\"tỷ\" + 0.016*\"bị_cáo\" + 0.014*\"năng_lượng\" + 0.014*\"triệu\" + 0.013*\"viễn\"'),\n",
       " (2,\n",
       "  '0.031*\"xe\" + 0.022*\"điện\" + 0.017*\"ôtô\" + 0.015*\"hệ_thống\" + 0.014*\"thiết_bị\" + 0.013*\"máy\" + 0.012*\"công_ty\" + 0.009*\"nhà_máy\" + 0.009*\"thiết_kế\" + 0.009*\"công_nghệ\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.save(PATH_DICTIONARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6764,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# gensim.corpora.MmCorpus.serialize(PATH_CORPUS, corpus=corpus)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m doc_topic_dist \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(\n\u001b[0;32m      5\u001b[0m             [[tup[\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m tup \u001b[39min\u001b[39;49;00m lst] \u001b[39mfor\u001b[39;49;00m lst \u001b[39min\u001b[39;49;00m lda_model[corpus]]\n\u001b[0;32m      6\u001b[0m         )\n\u001b[0;32m      7\u001b[0m \u001b[39m# save documents-topics matrix\u001b[39;00m\n\u001b[0;32m      8\u001b[0m joblib\u001b[39m.\u001b[39mdump(doc_topic_dist, PATH_TOPICS_DOCS_DIST)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6764,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import joblib\n",
    "# gensim.corpora.MmCorpus.serialize(PATH_CORPUS, corpus=corpus)\n",
    "doc_topic_dist = np.array(\n",
    "            [[tup[1] for tup in lst] for lst in lda_model[corpus]]\n",
    "        )\n",
    "# save documents-topics matrix\n",
    "joblib.dump(doc_topic_dist, PATH_TOPICS_DOCS_DIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
